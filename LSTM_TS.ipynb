{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM6REuPnql7wRxvrWLJVLnW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Jyoti1706/Data-Science-Project/blob/master/LSTM_TS.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "a49tDjolr8-_",
        "outputId": "23e1e5ce-5ee5-4a60-8887-7d95fa1c7b7e"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "        <script type=\"text/javascript\">\n",
              "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
              "        if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
              "        if (typeof require !== 'undefined') {\n",
              "        require.undef(\"plotly\");\n",
              "        requirejs.config({\n",
              "            paths: {\n",
              "                'plotly': ['https://cdn.plot.ly/plotly-2.24.1.min']\n",
              "            }\n",
              "        });\n",
              "        require(['plotly'], function(Plotly) {\n",
              "            window._Plotly = Plotly;\n",
              "        });\n",
              "        }\n",
              "        </script>\n",
              "        "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "        <script type=\"text/javascript\">\n",
              "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
              "        if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
              "        if (typeof require !== 'undefined') {\n",
              "        require.undef(\"plotly\");\n",
              "        requirejs.config({\n",
              "            paths: {\n",
              "                'plotly': ['https://cdn.plot.ly/plotly-2.24.1.min']\n",
              "            }\n",
              "        });\n",
              "        require(['plotly'], function(Plotly) {\n",
              "            window._Plotly = Plotly;\n",
              "        });\n",
              "        }\n",
              "        </script>\n",
              "        "
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "from numpy.random import seed\n",
        "seed(1)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_data(trainpath, testpath):\n",
        "    import pandas as pd\n",
        "    train = pd.read_csv('data/train.csv', parse_dates=['date'])\n",
        "    test = pd.read_csv('data/test.csv', parse_dates=['date'])\n",
        "    train.to_csv(f'data/train_df.csv', index=False)\n",
        "    test.to_csv(f'data/test_df.csv', index=False)\n",
        "    print(\"\\n ---- data csv is saved to PV location /data/train_df.csv ----\")\n",
        "    daily_sales = train.groupby('date', as_index=False)['sales'].sum()\n",
        "    store_daily_sales = train.groupby(['store', 'date'], as_index=False)['sales'].sum()\n",
        "    item_daily_sales = train.groupby(['item', 'date'], as_index=False)['sales'].sum()\n",
        "    store_daily_sales_sc = []\n",
        "    for store in store_daily_sales['store'].unique():\n",
        "      current_store_daily_sales = store_daily_sales[(store_daily_sales['store'] == store)]\n",
        "    item_daily_sales_sc = []\n",
        "    for item in item_daily_sales['item'].unique():\n",
        "        current_item_daily_sales = item_daily_sales[(item_daily_sales['item'] == item)]\n",
        "    train = train[(train['date'] >= '2017-01-01')]\n",
        "    train_gp = train.sort_values('date').groupby(['item', 'store', 'date'], as_index=False)\n",
        "    train_gp = train_gp.agg({'sales':['mean']})\n",
        "    train_gp.columns = ['item', 'store', 'date', 'sales']\n",
        "    train_gp.head()\n",
        "    def series_to_supervised(data, window=1, lag=1, dropnan=True):\n",
        "        cols, names = list(), list()\n",
        "        # Input sequence (t-n, ... t-1)\n",
        "        for i in range(window, 0, -1):\n",
        "            cols.append(data.shift(i))\n",
        "            names += [('%s(t-%d)' % (col, i)) for col in data.columns]\n",
        "        # Current timestep (t=0)\n",
        "        cols.append(data)\n",
        "        names += [('%s(t)' % (col)) for col in data.columns]\n",
        "\n",
        "        # Target timestep (t=lag)\n",
        "        cols.append(data.shift(-lag))\n",
        "        names += [('%s(t+%d)' % (col, lag)) for col in data.columns]\n",
        "\n",
        "        # Put it all together\n",
        "        agg = pd.concat(cols, axis=1)\n",
        "        agg.columns = names\n",
        "\n",
        "        # Drop rows with NaN values\n",
        "        if dropnan:\n",
        "            agg.dropna(inplace=True)\n",
        "        return agg\n",
        "    window = 29\n",
        "    lag_size = (test['date'].max().date() - train['date'].max().date()).days\n",
        "    lag = lag_size\n",
        "    series = series_to_supervised(train_gp.drop('date', axis=1), window=window, lag=lag)\n",
        "    series.head()\n",
        "    last_item = 'item(t-%d)' % window\n",
        "    last_store = 'store(t-%d)' % window\n",
        "    series = series[(series['store(t)'] == series[last_store])]\n",
        "    series = series[(series['item(t)'] == series[last_item])]\n",
        "    columns_to_drop = [('%s(t+%d)' % (col, lag)) for col in ['item', 'store']]\n",
        "    for i in range(window, 0, -1):\n",
        "        columns_to_drop += [('%s(t-%d)' % (col, i)) for col in ['item', 'store']]\n",
        "    series.drop(columns_to_drop, axis=1, inplace=True)\n",
        "    series.drop(['item(t)', 'store(t)'], axis=1, inplace=True)\n",
        "    series.to_csv(f'data/final_df.csv', index=False)\n"
      ],
      "metadata": {
        "id": "rnvBHR_PsjqQ"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Train Test Split\n",
        "# Label\n",
        "def train_test_split():\n",
        "    import pandas as pd\n",
        "    import numpy as np\n",
        "    from sklearn.model_selection import train_test_split\n",
        "    print(\"---- Inside train_test_split component ----\")\n",
        "    series = pd.read_csv(f'data/final_df.csv')\n",
        "    train = pd.read_csv(f'data/train_df.csv', parse_dates=['date'])\n",
        "    test = pd.read_csv(f'data/test_df.csv', parse_dates=['date'])\n",
        "    lag_size = (test['date'].max().date() - train['date'].max().date()).days\n",
        "    labels_col = 'sales(t+%d)' % lag_size\n",
        "    labels = series[labels_col]\n",
        "    series = series.drop(labels_col, axis=1)\n",
        "\n",
        "    X_train, X_valid, Y_train, Y_valid = train_test_split(series, labels.values, test_size=0.4, random_state=0)\n",
        "    print('Train set shape', X_train.shape)\n",
        "    print('Validation set shape', X_valid.shape)\n",
        "    np.save(f'data/X_train.npy', X_train)\n",
        "    np.save(f'data/X_valid.npy', X_valid)\n",
        "    np.save(f'data/y_train.npy', Y_train)\n",
        "    np.save(f'data/y_valid.npy', Y_valid)"
      ],
      "metadata": {
        "id": "aIUReMKjucHG"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training LSTM Model\n",
        "def trainModel():\n",
        "    import pandas as pd\n",
        "    import numpy as np\n",
        "    from tensorflow import keras\n",
        "    from tensorflow.keras import optimizers\n",
        "    from tensorflow.keras.models import Sequential, Model\n",
        "    from tensorflow.keras.layers import Dense, LSTM\n",
        "    epochs = 40\n",
        "    lr = 0.0003\n",
        "    adam = optimizers.Adam(lr)\n",
        "    X_train = np.load(f'data/X_train.npy',allow_pickle=True)\n",
        "    y_train = np.load(f'data/y_train.npy',allow_pickle=True)\n",
        "    X_valid = np.load(f'data/X_valid.npy',allow_pickle=True)\n",
        "    y_valid = np.load(f'data/y_valid.npy',allow_pickle=True)\n",
        "    X_train_series = X_train.values.reshape((X_train.shape[0], X_train.shape[1], 1))\n",
        "    X_valid_series = X_valid.values.reshape((X_valid.shape[0], X_valid.shape[1], 1))\n",
        "    print('Train set shape', X_train_series.shape)\n",
        "    print('Validation set shape', X_valid_series.shape)\n",
        "    model_lstm = Sequential()\n",
        "    model_lstm.add(LSTM(50, activation='relu', input_shape=(X_train_series.shape[1], X_train_series.shape[2])))\n",
        "    model_lstm.add(Dense(1))\n",
        "    model_lstm.compile(loss='mse', optimizer=adam)\n",
        "    #model_lstm.summary()\n",
        "    lstm_history = model_lstm.fit(X_train_series, y_train, validation_data=(X_valid_series, y_valid), epochs=epochs, verbose=2)\n",
        "    model_lstm.save(f'data/model_lstm.h5')\n"
      ],
      "metadata": {
        "id": "LfawDDnyumq0"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_on_test_data():\n",
        "    import pandas as pd\n",
        "    import numpy as np\n",
        "    import pickle\n",
        "    print(\"---- Inside predict_on_test_data component ----\")\n",
        "    X_train = np.load(f'data/X_train.npy',allow_pickle=True)\n",
        "    y_train = np.load(f'data/y_train.npy',allow_pickle=True)\n",
        "    X_valid = np.load(f'data/X_valid.npy',allow_pickle=True)\n",
        "    y_valid = np.load(f'data/y_valid.npy',allow_pickle=True)\n",
        "    X_train_series = X_train.values.reshape((X_train.shape[0], X_train.shape[1], 1))\n",
        "    X_valid_series = X_valid.values.reshape((X_valid.shape[0], X_valid.shape[1], 1))\n",
        "    model_lstm = keras.models.load_model(f'data/model_lstm.h5')\n",
        "    lstm_train_pred = model_lstm.predict(X_train_series)\n",
        "    lstm_valid_pred = model_lstm.predict(X_valid_series)\n",
        "    print('Train rmse:', np.sqrt(mean_squared_error(y_train, lstm_train_pred)))\n",
        "    print('Validation rmse:', np.sqrt(mean_squared_error(y_valid, lstm_valid_pred)))"
      ],
      "metadata": {
        "id": "qJr-kJve0zyN"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# KF Pipeline"
      ],
      "metadata": {
        "id": "SnMTehWI2-yp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#pip install kfp==1.8.22"
      ],
      "metadata": {
        "id": "oPc3Mvxy3MHW"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import kfp\n",
        "import kfp.components as comp\n",
        "import requests\n",
        "import kfp.dsl as dsl"
      ],
      "metadata": {
        "id": "scL_UaXU3Geb"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "create_step_prepare_data = kfp.components.create_component_from_func(\n",
        "    func=prepare_data,\n",
        "    base_image='python:3.7',\n",
        "    packages_to_install=['pandas==1.2.4','numpy==1.21.0', 'scikit-learn==0.24.2']\n",
        ")\n",
        "create_step_train_test_split = kfp.components.create_component_from_func(\n",
        "    func=train_test_split,\n",
        "    base_image='python:3.7',\n",
        "    packages_to_install=['pandas==1.2.4','numpy==1.21.0','scikit-learn==0.24.2']\n",
        ")\n",
        "create_step_training_model = kfp.components.create_component_from_func(\n",
        "    func=trainModel,\n",
        "    base_image='python:3.7',\n",
        "    packages_to_install=['pandas==1.2.4','numpy==1.21.0','scikit-learn==0.24.2']\n",
        ")\n",
        "create_step_predict_on_test_data = kfp.components.create_component_from_func(\n",
        "    func=predict_on_test_data,\n",
        "    base_image='python:3.7',\n",
        "    packages_to_install=['pandas==1.2.4','numpy==1.21.0','scikit-learn==0.24.2']\n",
        ")"
      ],
      "metadata": {
        "id": "U9m0B00yvam9"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the pipeline\n",
        "@dsl.pipeline(\n",
        "   name='Time Series Forecasting Kubeflow Demo Pipeline',\n",
        "   description='A sample pipeline that performs Time Series Forecasting'\n",
        ")\n",
        "# Define parameters to be fed into pipeline\n",
        "def TimeSeries_LSTM_pipeline(data_path: str):\n",
        "    vop = dsl.VolumeOp(\n",
        "    name=\"t-vol\",\n",
        "    resource_name=\"t-vol\",\n",
        "    size=\"2Gi\",\n",
        "    modes=dsl.VOLUME_MODE_RWO)\n",
        "\n",
        "    prepare_data_task = create_step_prepare_data(    testpath = \"data/test.csv\",\n",
        "    trainpath = \"data/train.csv\").add_pvolumes({data_path: vop.volume})\n",
        "    train_test_split = create_step_train_test_split().add_pvolumes({data_path: vop.volume}).after(prepare_data_task)\n",
        "    Model_training = create_step_training_model().add_pvolumes({data_path: vop.volume}).after(train_test_split)\n",
        "    log_predicted_class = create_step_predict_on_test_data().add_pvolumes({data_path: vop.volume}).after(Model_training)\n",
        "\n",
        "\n",
        "    prepare_data_task.execution_options.caching_strategy.max_cache_staleness = \"P0D\"\n",
        "    train_test_split.execution_options.caching_strategy.max_cache_staleness = \"P0D\"\n",
        "    Model_training.execution_options.caching_strategy.max_cache_staleness = \"P0D\"\n",
        "    log_predicted_class.execution_options.caching_strategy.max_cache_staleness = \"P0D\"\n",
        "\n",
        ""
      ],
      "metadata": {
        "id": "5LgFMp2V3wGk"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "kfp.compiler.Compiler().compile(\n",
        "    pipeline_func=TimeSeries_LSTM_pipeline,\n",
        "    package_path='TimeSeries_LSTM_pipeline1.yaml')"
      ],
      "metadata": {
        "id": "eNGWrMtM4sNO"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AuKuNjew47TI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}